---
title: "Minería de textos en libros"
author: "Roberto Muñoz"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    toc: true
    toc_depth: 2
    #number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ajustamos el locale del sistema de acuerdo al OS del computador
En caso de usar un Mac ejecute la siguiente linea
```{r}
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
```

En caso de usar Windows ejecute la siguiente linea
```{r}
Sys.setlocale("LC_ALL", 'Spanish_Chile.1252')
```

# Cargamos las librerías que usaremos en el script
```{r}
library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(dplyr)
library(readr)
library(cluster)
```

# Lectura y preparación del texto
El texto con el que trabajeremos es el texto del libro Niebla de Miguel de Unamuno, el cual puede ser descargado de la biblioteca digital Gutenberg.

El libro Niebla fue publicado el año 1914 y cuenta ls historia de Augusto Pérez, una persona que tiene una vida muy rutinaria y tranquila. Comienza a cuestionar cada uno de los aspectos de su vida: se pregunta si alguien sabe lo que es amar, qué es vivir y cuál es la finalidad de la existencia, entre otros temas. 

Nuestro interés está en el contenido de este libro y no en los avisos legales de Gutenberg, prólogo y anotaciones, así que los omitiremos de la lectura. Empezaremos a leer el documento desde la línea 420, que es donde termina el prólogo, introducción e índice de Niebla, para ello nos saltaremos (skip) 419 líneas previas. De manera complementaria, detendremos la lectura en la línea 8313, que es donde inician los avisos legales de Gutenberg, por lo tanto leeremos un máximo (nmax) de 8313-419 líneas.

Realizamos estas operaciones y asignamos el resultado al objeto libro_raw.

```{r}
libro_raw <- read_lines("https://github.com/rpmunoz/topicos_ingenieria_2/raw/master/clase_3/data/Niebla-Miguel_de_unamuno.txt", skip = 419, n_max = 8313-419)
```

El objeto libro_raw que obtuvimos es del tipo character con 7894 elementos.

```{r}
str(libro_raw)
```

Cada uno de estos elementos corresponde a una línea de Niebla y tiene ancho máximo 70 caracteres, pues este es el ancho usado en los textos electrónicos de Gutenberg. Esta es una cantidad muy pequeña de texto para encontrar asociaciones entre palabras, por lo que necesitamos crear elementos con una mayor cantidad de caracteres en cada uno.

## Creación de párrafos

Una manera de crear párrafos completos es editando el texto original y eliminando los saltos de línea. Existen más de 2 mil párrafos en el texto, por lo cual nos tomaría tiempo editarlo manualmente.

Otra manera es unir 10 líneas y crear párrafos.  Creamos un vector llamado diez con 10 repeticiones de los números desde 1 hasta el número de líneas en el documento, dividido entre 10 (length(nov_raw)/10.

Con esto, tendremos un vector con diez 1, luego diez 2, etc, hasta llegar al número máximo de grupos de diez posibles en función del número de líneas de nuestro documento.

Usaremos estos números para hacer grupos de diez líneas consecutivas.

```{r}
diez <- rep(1:ceiling(length(libro_raw)/10), each = 10)
diez
```

De este vector, nos quedamos con un número de elementos igual al número de líneas del objeto nov_raw (length(nov_raw)), para facilitar combinarlos.

```{r}
diez <- diez[1:length(libro_raw)]
diez
```

Combinamos diez con libro_raw y los asignamos al objeto libro_text. Así tenemos una columna con las líneas de texto y otra con un número que identifica a qué grupo de diez líneas pertenece.

Además, convertimos a data.frame para que las columnas estén identificadas con un nombre, lo cual será útil en los siguientes pasos.

```{r}
libro_text <- cbind(diez, libro_raw) %>% data.frame()
libro_text
```

Usamos aggregate para concatenar las líneas (FUN = paste, con collapse = " " para preservar el espacio entre palabras), agrupados por diez (formula = libro_raw ~ diez).

```{r}
libro_text <- aggregate(formula = libro_raw ~ diez,
                      data = libro_text,
                      FUN = paste,
                      collapse = " ")
View(libro_text)
```

Como sólo necesitamos la columna con los ahora párrafos de texto, extraemos solola columna libro_raw. Aprovechamos para transformar libro_text en una matriz, pues esto nos facilitará los pasos siguientes. En total debiésemos tener 790 párrafos.

```{r}
libro_text %>%
  select(libro_raw) %>%
  as.matrix -> libro_text
dim(libro_text)
```

## Limpieza del texto

Necesitamos limpiar el texto de caracteres que son de poca utilidad en la mineria de textos.

Empezamos por aseguramos de que no queden caracteres especiales de la codificación, como saltos de línea y tabulaciones, con un poco de ayuda de Regular Expressions.

```{r}
libro_text <- gsub("[[:cntrl:]]", " ", libro_text)
libro_text <- tolower(libro_text)
libro_text <- removeWords(libro_text, words = stopwords("spanish"))
libro_text <- removePunctuation(libro_text)
libro_text <- removeNumbers(libro_text)
libro_text <- stripWhitespace(libro_text)
```

Visualizamos el texto luego de haberlo limpiado

```{r}
View(libro_text)
```


## Análisis del Corpus

Con nuestro documento preparado, procedemos a crear nuestro Corpus, es decir, esto es nuestro acervo de documentos a analizar.

En nuestro caso, nuestro Corpus se compone de todos los parrafos del libro Niebla y los asignaremos al objeto libro_corpus usando las funciones VectorSource y Corpus.

```{r}
libro_corpus <- Corpus(VectorSource(libro_text))
libro_corpus
```

Como podemos ver, nuestro Copus está compuesto por 790 documentos. Los siguientes análisis se harán a partir de este Corpus.

## Nube de palabras

Para analizar datos en forma de texto, usamos una representación del tipo Document-Term Matrix (DTM)

```{r}
libro_dtm <- DocumentTermMatrix(libro_corpus)
libro_dtm
```

Para reducir la dimnesionalidad de la matriz DTM, podemos eliminar las palabras menos frecuentes. Para ellos usamos la función removeSparseTerms() y el valor 99 para la dispersión.

```{r}
libro_dtm = removeSparseTerms(libro_dtm, 0.99)
libro_dtm
```

Usemos la función wordcloud() para mostrar una nube de palabras

```{r}
freq = data.frame(sort(colSums(as.matrix(libro_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=80, random.order = F, colors=brewer.pal(8, "Dark2"))
```

## Más depuración

Como observamos en las nubes de palabras que generamos, aún tenemos palabras que aparecen con mucha frecuencia en nuestro texto que en realidad no son de mucha utilidad para el análisis.

Usaremos la función removeWords indicando en el argumento words que palabras deseamos eliminar de nuestro Corpus.

```{r}
libro_text <- removeWords(libro_text, words = c("usted", "pues", "tal", "tan", "así", "dijo", "cómo", "sino", "entonces", "aunque", "don", "doña"))

libro_corpus <- libro_text %>% VectorSource() %>% Corpus()
```

```{r}
libro_dtm <- DocumentTermMatrix(libro_corpus)
libro_dtm = removeSparseTerms(libro_dtm, 0.99)
libro_dtm

freq = data.frame(sort(colSums(as.matrix(libro_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=80, random.order = F, colors=brewer.pal(8, "Dark2"))
```

## Term Document Matrix y Frecuencia de palabras

Mapearemos nuestro Corpus indicando que es una matriz de términos, de esta manera podremos hacer realizar operaciones como identificar asociaciones entre palabras.

Usaremos la función TermDocumentMatrix en nuestro Corpus y asignaremos el resultado al objeto libro_tdm.

```{r}
libro_tdm <- TermDocumentMatrix(libro_corpus)
libro_tdm
```

Creamos la variable libro_mat que almacena libro_tdm como matriz

```{r}
libro_mat <- as.matrix(libro_tdm)
dim(libro_mat)
View(libro_mat)
```

Obtenemos las sumas de líneas (rowSums) odenadas de mayor a menor (sort con decreasing = TRUE) para conocer la frecuencia de cada palabra y después transformamos los resultados a objeto de clase data.frame de dos columnas, palabra y frec, que nos permitirá graficar fácilmente su contenido.

```{r}
libro_mat %>% rowSums() %>% sort(decreasing = TRUE) -> libro_mat
data.frame(palabra = names(libro_mat), frec = libro_mat) -> libro_mat
View(libro_mat)
```

Además, podemos obtener fácilmente las palabras más frecuentes. Por ejemplo, las veinte más frecuentes, pidiendo los primeras veinte líneas de libro_mat.

```{r}
libro_mat[1:20, ]
```

## Gráficas de frecuencia

Crearemos un par de gráficas. Primero, la frecuencia de uso de las palabras más frecuentes en Niebla.

Para esto usaremos ggplot2, que tiene su propia gramática para construir gráficas. Para fines de este documento, no nos detendremos a explicar a detalle su uso. Lo relevante aquí es notar que estamos obteniendo la información para construir las gráficas solicitando renglones del objeto nov_mat

```{r}
libro_mat[1:10, ] %>%
  ggplot(aes(palabra, frec)) +
  geom_bar(stat = "identity", color = "black", fill = "#87CEFA") +
  geom_text(aes(hjust = 1.3, label = frec)) + 
  coord_flip() + 
  labs(title = "Diez palabras más frecuentes en Niebla",  x = "Palabras", y = "Número de usos")
```

La misma información expresada como proporción de uso cada palabra. Para esta gráfica usamos la función mutate de dplyr para obtener el porcentaje de uso de cada palabra antes de graficar.

```{r}
libro_mat %>%
  mutate(perc = (frec/sum(frec))*100) %>%
  .[1:10, ] %>%
  ggplot(aes(palabra, perc)) +
  geom_bar(stat = "identity", color = "black", fill = "#87CEFA") +
  geom_text(aes(hjust = 1.3, label = round(perc, 2))) + 
  coord_flip() +
  labs(title = "Diez palabras más frecuentes en Niebla", x = "Palabras", y = "Porcentaje de uso")
```

## Asociaciones entre palabras

Veamos ahora cómo se asocian algunas palabras (terms) en Niebla con la función findAssocs. Como podemos introducir un vector, podemos obtener las asociaciones de varias palabras a la vez. He elegido "augusto", "eugenia", "hombre" y "mujer".

Es importante recordar que con esto no estamos pidiendo la asociacion de estas cuatro palabras entre si, sino las asociaciones para cada una de las cuatro, que no necesariamente deben coincidir.

Esta también nos pide el límite inferior de correlación (corlimit) para mostrarnos. Valores cercanos a 1 indican que las palabras aparecen casi siempre asociadas una con otra, valores cercanos a 0 nos indican que nunca o casi nunca lo hacen.

El valor que decidamos depende del tipo de documento y el tipo de asociaciones que nos interesen. para nuestros fines, lo he fijado en .25.

```{r}
findAssocs(libro_tdm, terms = c("augusto", "eugenia", "hombre", "mujer"), corlimit = .25)
```

Aunque "augusto" es la palabra más frecuente en Niebla no tiene relaciones tan fuertes como las demás palabras que elegimos. De hecho, no tiene ninguna superior a .25, por eso el resultado mostrado.

A partir de estos resultados podemos observar algunas cosas, por ejemplo, que "hombre" está asociada con adjetivos negativos, que "mujer" se asocia con palabras que hacen alusión a fuego y explosiones, y que "lucharemosiba" aparece como una palabra, que nos indica que sería buena idea realizar aún más limpieza en nuestro corpus.

# Agrupamiento jerárquico (Hierarchical clustering)

Realizaremos análisis de agrupaciones jerárquicas para identificar grupos de palabras relacionados entre sí, a partir de la distancia que existe entre ellos.

Empezaremos por eliminar los términos dispersos en nuestra matriz de términos, para así conservar únicamente las palabras más frecuentes y obtener resultados más interpretables del agrupamiento.

## Eliminar términos dispersos

Usaremos las función removeSparseItems para depurar nuestra matriz de términos de aquellas palabras que aparecen con muy poca frecuencia, es decir, son dispersos ("sparse").

Qué valor fijemos depende del tipo de documento que tengamos, por lo que es aconsejable realizar ensayos hasta encontrar un equilibrio entre dispersión y número de términos. En este caso, he decidido fijarlo en .95 y guardaremos la nueva matriz de términos en el objeto nov_new

```{r}
libro_new <- removeSparseTerms(libro_tdm, sparse = .95)
```

Comparamos cuántos términos teníamos originalmente y con cuántos nos hemos quedado, observando a cuánto equivale terms.

```{r}
libro_tdm
```

```{r}
libro_new
```

De 6927 términos que teníamos, nos hemos quedado con 45, lo cual reduce en gran medida la dificultad y complejidad de los agrupamientos, lo cual es deseable. Es poco útil tener agrupaciones que son únicamente visualizaciones del texto original.

También podemos ver el número de términos pidiéndo el número de renglones de nuestra matriz de términos, que es igual al número de palabras que contiene.

```{r}
libro_tdm$nrow
```

```{r}
libro_new$nrow
```

Transformamos esta matriz de términos a un objeto de tipo matrix para así poder realizar las operaciones posteriores.

```{r}
libro_new <- libro_new %>% as.matrix()
```

# Matriz de distancia

Necesitamos crear una matriz de distancias para empezar agrupar, lo cual requiere que los valores en las celdas sean estandarizados de alguna manera.

Podríamos usar la función scale, pero realiza la estandarización usando la media de cada columna como referencia, mientras que nosotros necesitamos como referencia la media de cada renglón.

Así que obtenemos una estandarización por renglones de manera manual.

```{r}
libro_new <- libro_new / rowSums(libro_new)
```

Hecho esto, nuestra matriz ha sido estandarizada.

Procedemos a obtener una matriz de distancia a partir de ella, con el método de distancias euclidianas y la asignamos al objeto nov_dist.

```{r}
libro_dist <- dist(libro_new, method = "euclidian")
```

# Clustering

Realizaremos nuestro agrupamiento jerárquico usando la función hclust, de la base de R. Este es en realidad un procedimiento muy sencillo una vez que hemos realizado la preparación.

Usaremos el método de Ward (ward.D), que es el método por defecto de la función hclust y asignaremos sus resultados al objeto libro_hclust.

```{r}
libro_hclust <-  hclust(libro_dist, method = "ward.D")
```

Graficamos los resultados usando plot para generar un dendrograma.

```{r}
plot(libro_hclust, main = "Dendrograma de Niebla - hclust", sub = "", xlab = "")
```

De este modo podemos observar los grupos de palabras que existen en Niebla. Por ejemplo, "augusto" y "eugenia" forman un grupo, "puede" y "ser", forman otro grupo ("puede ser" es una frase común en este libro).

Además, podemos ver qué palabras pertenecen a grupos lejanos entre sí, por ejemplo, "quiero" y "verdad".

Podemos enfatizar los grupos de palabras trazando un rectángulo usando rect.hclust y con especificando cuántos grupos (k) deseamos resaltar.

Crearemos el mismo gráfico pidiendo diez grupos.

```{r}
{plot(libro_hclust, main = "Dendrograma de Niebla - hclust", sub = "", xlab = "")
rect.hclust(libro_hclust, k = 10, border="blue")}
```

# Agnes (Agglomerative Nesting)

El paquete cluster nos proporciona más métodos para realizar agrupamientos. Uno de ellos es agnes, que inicia asumiendo que cada elemento a agrupar por si mismo es un grupo y después crea grupos de grupos a partir de las distancias entre ellos, hasta que no es posible crear más grupos.

Realizamos prácticamente el mismo procedimiento que con hclust, sólo cambiamos el método a average. Asignaremos nuestros resultados al objeto libro_agnes.

```{r}
libro_agnes <- agnes(libro_dist, method = "average")
```

Ahora graficamos nuestros resultados. Un agrupamiento creado con agnes genera dos gráficos, el primero muestra cómo se obtuvieron los grupos finales y el segundo es un dendrograma.

Pediremos el segundo gráfico (which.plots = 2).

```{r}
plot(libro_agnes, which.plots = 2, main = "Dendrograma de Niebla - Agnes", sub = "", xlab = "")
```

Enfatizamos diez grupos.

```{r}
{plot(libro_agnes, which.plots = 2, main = "Dendrograma de Niebla - Agnes", sub = "", xlab = "")
rect.hclust(libro_agnes, k = 10, border = "blue")}
```

Las agrupaciones que hemos obtenido usando hclust y agnes son diferentes entre sí. La decisión de qué método usemos depende de nuestros propósitos y de nuestra familiaridad con ellos.


